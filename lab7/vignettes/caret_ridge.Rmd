---
title: <center>Ridgereg_caret Vignette </center>
subtitle: <center>farch587@student.liu.se, syesh076@student.liu.se</center>
author: <center>Farhana Chowdhury Tondra, Syeda Farha Shazmeen </center>
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
 %\VignetteIndexEntry{ridgereg}
  %\VignetteEngine{knitr::knitr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}{inputenc}
  
---
```{r}
library(mlbench)
library(caret)
library(leaps)
library(lab7)
data("BostonHousing")
```
The caret package is used for classification and regression.

## 1. Divide the BostonHousing data (or your own API data) into a test and training dataset using the caret package.

Dividing the BostonHousing data into 70% training and 30% test data 


```{r}
library(mlbench)
library(caret)
data("BostonHousing")
names(BostonHousing)
train_data <- caret::createDataPartition(BostonHousing$age, p = .7,
                                         list = FALSE,
                                         times= 1)
Trainingdata <- BostonHousing[train_data, ]
Testdata <- BostonHousing[-train_data, ]


nrow(Trainingdata)
nrow(Testdata)
```



## 2. Fit a linear regression model and a linear regression model with forward selection of covariates on the training dataset.

The first two arguments of the train function are **predictor** and **outcome** data objects.Traincontrol object in train allows us to specify the resampling method.The resampling method used here is **cv-cross validation**. <br/>
For our analysis we have taken crim variable . R-squared  and RMSE(Root Mean Square Error)is used measure model performance. RSME values is low with  5.706505.

Fit a linear regression model: 

```{r}
ridge <- caret::train(crim~.,
                      data = Trainingdata,
                      method='lm',
                      trControl = trainControl(method = "cv")
)

print(ridge)
```


Fitting a linear model with method = leapForward on the training dataset :

```{r}
lflmGrid <- expand.grid(nvmax=1:(ncol(Trainingdata)-1))
ridge <- caret::train(crim~.,
                      data = Trainingdata,
                      method='leapForward',
                      tuneGrid = lflmGrid
)
print(ridge)
```

## 3. Evaluate the performance of this model on the training dataset.

Since **the RMSE & MAE is low on training of lm model compared to leapForward lm where model has good perfomance with nvmax(number of predictors) we can conclude that LM is better than leapforward LM.**

##4. Fit a ridge regression model using your ridgereg() function to the training dataset for different values of lambda.


```{r}
ridge <- list(type="Regression", 
              library="lab7",
              loop=NULL,
              prob=NULL)

ridge$parameters <- data.frame(parameter="lambda",
                               class="numeric",
                               label="lambda")


 ridge$grid <- function (x, y, len = NULL, search = "grid"){
    data.frame(lambda = lambda)
  } 
  
  ridge$fit <- function (x, y, wts, param, lev, last, classProbs, ...) {
    dat <- if (is.data.frame(x)) 
      x
    else as.data.frame(x)
    dat$.outcome <- y
    out <- ridgereg$new(.outcome ~ ., data=dat ,lambda = param$lambda, normalize=normalize, ...)
    
    out
  }
  
  ridge$predict <- function (modelFit, newdata, submodels = NULL) {
    if (!is.data.frame(newdata)) 
      newdata <- as.data.frame(newdata)
    newdata <- scale(newdata)
    modelFit$predict(newdata)
  }

```



###5. Find the best hyperparameter value for lambda using 10-fold cross-validation on the training set.

```{r echo=FALSE}
fitControl <- caret::trainControl(method = "cv",
                                    number = 10)
 lambdaGrid <- expand.grid(lambda = c(0,.01,.02,.03,.04))
  ridge <- caret::train(crim~.,
                        data = Trainingdata,
                        method='ridge',
                        trControl = fitControl,
                        tuneGrid = lambdaGrid,
                        preProcess=c('center', 'scale')
  )
  predict(ridge$finalModel, type='coef', mode='norm')$coefficients[13,]
  ridge.pred <- predict(ridge, Testdata)
  avgErrror<-2*sqrt(mean(ridge.pred - Testdata$crim)^2)
  print(ridge)

```

So, **the best hyperparameter value for lambda is 0.03**



## 6. Evaluate the performance of all three models on the test dataset.

After evaluating datasets, it can be stated that **ridge regression model is better than lm and lm leapforward regression as it gives lower RMSE.**

